{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7e2e158",
      "metadata": {
        "id": "f7e2e158"
      },
      "source": [
        "![ViT](./media/img_demo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31966e3a",
      "metadata": {
        "id": "31966e3a"
      },
      "source": [
        "# Visualize Vision Transformer Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "bee45a8f",
      "metadata": {
        "id": "bee45a8f"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CG4MBmpJYK04",
      "metadata": {
        "id": "CG4MBmpJYK04"
      },
      "outputs": [],
      "source": [
        "# import ipywidgets as widgets\n",
        "import io\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def transform(img, img_size):\n",
        "    img = transforms.Resize(img_size)(img)\n",
        "    img = transforms.ToTensor()(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def visualize_predict(model, img, img_size, patch_size, device):\n",
        "    img_pre = transform(img, img_size)\n",
        "    attention = visualize_attention(model, img_pre, patch_size, device)\n",
        "    plot_attention(img, attention)\n",
        "    return attention\n",
        "\n",
        "\n",
        "def visualize_attention(model, img, patch_size, device):\n",
        "    # make the image divisible by the patch size\n",
        "    w, h = (\n",
        "        img.shape[1] - img.shape[1] % patch_size,\n",
        "        img.shape[2] - img.shape[2] % patch_size,\n",
        "    )\n",
        "    img = img[:, :w, :h].unsqueeze(0)\n",
        "\n",
        "    w_featmap = img.shape[-2] // patch_size\n",
        "    h_featmap = img.shape[-1] // patch_size\n",
        "\n",
        "    attentions = model.get_last_selfattention(img.to(device))\n",
        "\n",
        "    nh = attentions.shape[1]  # number of head\n",
        "\n",
        "    # keep only the output patch attention\n",
        "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
        "\n",
        "    attentions = attentions.reshape(nh, w_featmap, h_featmap)\n",
        "    attentions = (\n",
        "        nn.functional.interpolate(\n",
        "            attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\"\n",
        "        )[0]\n",
        "        .cpu()\n",
        "        .numpy()\n",
        "    )\n",
        "\n",
        "    return attentions\n",
        "\n",
        "\n",
        "def plot_attention(img, attention):\n",
        "    n_heads = attention.shape[0]\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    text = [\"Original Image\", \"Head Mean\"]\n",
        "    for i, fig in enumerate([img, np.mean(attention, 0)]):\n",
        "        plt.subplot(1, 2, i + 1)\n",
        "        plt.imshow(fig, cmap=\"inferno\")\n",
        "        plt.title(text[i])\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for i in range(n_heads):\n",
        "        plt.subplot(n_heads // 3, 3, i + 1)\n",
        "        plt.imshow(attention[i], cmap=\"inferno\")\n",
        "        plt.title(f\"Head n: {i+1}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "class Loader(object):\n",
        "    def __init__(self):\n",
        "        self.uploader = widgets.FileUpload(accept=\"/content/image*\", multiple=False)\n",
        "        self._start()\n",
        "\n",
        "    def _start(self):\n",
        "        display(self.uploader)\n",
        "\n",
        "    def getLastImage(self):\n",
        "        try:\n",
        "            for uploaded_filename in self.uploader.value:\n",
        "                uploaded_filename = uploaded_filename\n",
        "            img = Image.open(\n",
        "                io.BytesIO(bytes(self.uploader.value[uploaded_filename][\"content\"]))\n",
        "            )\n",
        "            return img\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "    def saveImage(self, path):\n",
        "        with open(path, \"wb\") as output_file:\n",
        "            for uploaded_filename in self.uploader.value:\n",
        "                content = self.uploader.value[uploaded_filename][\"content\"]\n",
        "                output_file.write(content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0a8412",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "C5sQkKrFYMfG",
      "metadata": {
        "id": "C5sQkKrFYMfG"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'GELU'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_65486/1380112409.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     def __init__(\n\u001b[1;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_65486/1380112409.py\u001b[0m in \u001b[0;36mMlp\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mhidden_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mout_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mact_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGELU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     ):\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'GELU'"
          ]
        }
      ],
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates & Vittorio Mazzia.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"\n",
        "Mostly copy-paste from timm library.\n",
        "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "\"\"\"\n",
        "import math\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def trunc_normal_(tensor, mean=0.0, std=1.0, a=-2.0, b=2.0):\n",
        "    # type: (Tensor, float, float, float, float) -> Tensor\n",
        "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
        "\n",
        "\n",
        "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
        "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
        "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    def norm_cdf(x):\n",
        "        # Computes standard normal cumulative distribution function\n",
        "        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob: float = 0.0, training: bool = False):\n",
        "    if drop_prob == 0.0 or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    # work with diff dim tensors, not just 2D ConvNets\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()  # binarize\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\"\"\"\n",
        "\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super(DropPath, self).__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        hidden_features=None,\n",
        "        out_features=None,\n",
        "        act_layer=nn.GELU,\n",
        "        drop=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        out_features = out_features or in_features\n",
        "        hidden_features = hidden_features or in_features\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = act_layer()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads=8,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim**-0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = (\n",
        "            self.qkv(x)\n",
        "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
        "            .permute(2, 0, 3, 1, 4)\n",
        "        )\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        num_heads,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop=0.0,\n",
        "        attn_drop=0.0,\n",
        "        drop_path=0.0,\n",
        "        act_layer=nn.GELU,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim,\n",
        "            num_heads=num_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            qk_scale=qk_scale,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=drop,\n",
        "        )\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(\n",
        "            in_features=dim,\n",
        "            hidden_features=mlp_hidden_dim,\n",
        "            act_layer=act_layer,\n",
        "            drop=drop,\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_attention=False):\n",
        "        y, attn = self.attn(self.norm1(x))\n",
        "        if return_attention:\n",
        "            return attn\n",
        "        x = x + self.drop_path(y)\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Image to Patch Embedding\"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        num_patches = (img_size // patch_size) * (img_size // patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Vision Transformer\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size=[224],\n",
        "        patch_size=16,\n",
        "        in_chans=3,\n",
        "        num_classes=0,\n",
        "        embed_dim=768,\n",
        "        depth=12,\n",
        "        num_heads=12,\n",
        "        mlp_ratio=4.0,\n",
        "        qkv_bias=False,\n",
        "        qk_scale=None,\n",
        "        drop_rate=0.0,\n",
        "        attn_drop_rate=0.0,\n",
        "        drop_path_rate=0.0,\n",
        "        norm_layer=nn.LayerNorm,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim\n",
        "\n",
        "        self.patch_embed = PatchEmbed(\n",
        "            img_size=img_size[0],\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=embed_dim,\n",
        "        )\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # stochastic depth decay rule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    num_heads=num_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    qk_scale=qk_scale,\n",
        "                    drop=drop_rate,\n",
        "                    attn_drop=attn_drop_rate,\n",
        "                    drop_path=dpr[i],\n",
        "                    norm_layer=norm_layer,\n",
        "                )\n",
        "                for i in range(depth)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = (\n",
        "            nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        )\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=0.02)\n",
        "        trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def interpolate_pos_encoding(self, x, w, h):\n",
        "        npatch = x.shape[1] - 1\n",
        "        N = self.pos_embed.shape[1] - 1\n",
        "        if npatch == N and w == h:\n",
        "            return self.pos_embed\n",
        "        class_pos_embed = self.pos_embed[:, 0]\n",
        "        patch_pos_embed = self.pos_embed[:, 1:]\n",
        "        dim = x.shape[-1]\n",
        "        w0 = w // self.patch_embed.patch_size\n",
        "        h0 = h // self.patch_embed.patch_size\n",
        "        # we add a small number to avoid floating point error in the interpolation\n",
        "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
        "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
        "        patch_pos_embed = nn.functional.interpolate(\n",
        "            patch_pos_embed.reshape(\n",
        "                1, int(math.sqrt(N)), int(math.sqrt(N)), dim\n",
        "            ).permute(0, 3, 1, 2),\n",
        "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
        "            mode=\"bicubic\",\n",
        "        )\n",
        "        assert (\n",
        "            int(w0) == patch_pos_embed.shape[-2]\n",
        "            and int(h0) == patch_pos_embed.shape[-1]\n",
        "        )\n",
        "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
        "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
        "\n",
        "    def prepare_tokens(self, x):\n",
        "        B, nc, w, h = x.shape\n",
        "        x = self.patch_embed(x)  # patch linear embedding\n",
        "\n",
        "        # add the [CLS] token to the embed patch tokens\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # add positional encoding to each token\n",
        "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
        "\n",
        "        return self.pos_drop(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.prepare_tokens(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.norm(x)\n",
        "        return x[:, 0]\n",
        "\n",
        "    def get_last_selfattention(self, x):\n",
        "        x = self.prepare_tokens(x)\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            if i < len(self.blocks) - 1:\n",
        "                x = blk(x)\n",
        "            else:\n",
        "                # return attention of the last block\n",
        "                return blk(x, return_attention=True)\n",
        "\n",
        "    def get_intermediate_layers(self, x, n=1):\n",
        "        x = self.prepare_tokens(x)\n",
        "        # we return the output tokens from the `n` last blocks\n",
        "        output = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x = blk(x)\n",
        "            if len(self.blocks) - i <= n:\n",
        "                output.append(self.norm(x))\n",
        "        return output\n",
        "\n",
        "\n",
        "class VitGenerator(object):\n",
        "    def __init__(\n",
        "        self, name_model, patch_size, device, evaluate=True, random=False, verbose=False\n",
        "    ):\n",
        "        self.name_model = name_model\n",
        "        self.patch_size = patch_size\n",
        "        self.evaluate = evaluate\n",
        "        self.device = device\n",
        "        self.verbose = verbose\n",
        "        self.model = self._getModel()\n",
        "        self._initializeModel()\n",
        "        if not random:\n",
        "            self._loadPretrainedWeights()\n",
        "\n",
        "    def _getModel(self):\n",
        "        if self.verbose:\n",
        "            print(\n",
        "                f\"[INFO] Initializing {self.name_model} with patch size of {self.patch_size}\"\n",
        "            )\n",
        "        if self.name_model == \"vit_tiny\":\n",
        "            model = VisionTransformer(\n",
        "                patch_size=self.patch_size,\n",
        "                embed_dim=192,\n",
        "                depth=12,\n",
        "                num_heads=3,\n",
        "                mlp_ratio=4,\n",
        "                qkv_bias=True,\n",
        "                norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "            )\n",
        "\n",
        "        elif self.name_model == \"vit_small\":\n",
        "            model = VisionTransformer(\n",
        "                patch_size=self.patch_size,\n",
        "                embed_dim=384,\n",
        "                depth=12,\n",
        "                num_heads=6,\n",
        "                mlp_ratio=4,\n",
        "                qkv_bias=True,\n",
        "                norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "            )\n",
        "\n",
        "        elif self.name_model == \"vit_base\":\n",
        "            model = VisionTransformer(\n",
        "                patch_size=self.patch_size,\n",
        "                embed_dim=768,\n",
        "                depth=12,\n",
        "                num_heads=12,\n",
        "                mlp_ratio=4,\n",
        "                qkv_bias=True,\n",
        "                norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "            )\n",
        "        else:\n",
        "            raise f\"No model found with {self.name_model}\"\n",
        "\n",
        "        return model\n",
        "\n",
        "    def _initializeModel(self):\n",
        "        if self.evaluate:\n",
        "            for p in self.model.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "            self.model.eval()\n",
        "\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def _loadPretrainedWeights(self):\n",
        "        if self.verbose:\n",
        "            print(\"[INFO] Loading weights\")\n",
        "        url = None\n",
        "        if self.name_model == \"vit_small\" and self.patch_size == 16:\n",
        "            url = \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == \"vit_small\" and self.patch_size == 8:\n",
        "            url = \"dino_deitsmall8_300ep_pretrain/dino_deitsmall8_300ep_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == \"vit_base\" and self.patch_size == 16:\n",
        "            url = \"dino_vitbase16_pretrain/dino_vitbase16_pretrain.pth\"\n",
        "\n",
        "        elif self.name_model == \"vit_base\" and self.patch_size == 8:\n",
        "            url = \"dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\"\n",
        "\n",
        "        if url is None:\n",
        "            print(\n",
        "                f\"Since no pretrained weights have been found with name {self.name_model} and patch size {self.patch_size}, random weights will be used\"\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            state_dict = torch.hub.load_state_dict_from_url(\n",
        "                url=\"https://dl.fbaipublicfiles.com/dino/\" + url\n",
        "            )\n",
        "            self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def get_last_selfattention(self, img):\n",
        "        return self.model.get_last_selfattention(img.to(self.device))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9367bd70",
      "metadata": {
        "id": "9367bd70"
      },
      "outputs": [],
      "source": [
        "# import some libraries\n",
        "import os\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "847ba643",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847ba643",
        "outputId": "6ef6baeb-3a20-4202-e7cc-188673d1c4b1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "if device.type == \"cuda\":\n",
        "    torch.cuda.set_device(1)\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d3f7c51",
      "metadata": {
        "id": "2d3f7c51"
      },
      "source": [
        "# 1.0 Load the ViT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925a24e9",
      "metadata": {
        "id": "925a24e9"
      },
      "outputs": [],
      "source": [
        "# set some variables\n",
        "name_model = 'vit_small'\n",
        "patch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24933397",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24933397",
        "outputId": "9b67bc7e-6831-4a38-ea99-d07375d34dcd"
      },
      "outputs": [],
      "source": [
        "model = VitGenerator(name_model, patch_size,\n",
        "                     device, evaluate=True, random=False, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cdff309",
      "metadata": {
        "id": "6cdff309"
      },
      "source": [
        "# 2.0 Visualize Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c41e23",
      "metadata": {
        "id": "04c41e23"
      },
      "source": [
        "## 2.1 Load an image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7102425",
      "metadata": {
        "heading_collapsed": true,
        "id": "f7102425"
      },
      "source": [
        "### 2.1.1 Use predefined path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4375391",
      "metadata": {
        "hidden": true,
        "id": "f4375391"
      },
      "outputs": [],
      "source": [
        "path = '/content/image/WhatsApp Image 2023-12-23 at 11.50.45 AM.jpeg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3fe1a78",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "hidden": true,
        "id": "c3fe1a78",
        "outputId": "facf9417-707c-4830-def7-d5cd759a58cf"
      },
      "outputs": [],
      "source": [
        "img = Image.open(path)\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5699378",
      "metadata": {
        "id": "f5699378"
      },
      "source": [
        "### 2.1.2 Use the loader widget"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "624c619c",
      "metadata": {
        "id": "624c619c"
      },
      "source": [
        "If it doesn't work, execute the following passage:\n",
        "- pip install ipywidgets (naturally in your terminal)\n",
        "- jupyter nbextension enable --py widgetsnbextension\n",
        "- reload notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd8a9fbb",
      "metadata": {
        "id": "bd8a9fbb"
      },
      "outputs": [],
      "source": [
        "factor_reduce = 2\n",
        "img_size = tuple(np.array(img.size[::-1]) // factor_reduce) # PIL works with col, row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d36577f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d36577f8",
        "outputId": "bc1b81fa-95f8-4a00-b5bb-7a149acd025a"
      },
      "outputs": [],
      "source": [
        "at = visualize_predict(model, img, img_size, patch_size, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5wTyND7oHte",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "f5wTyND7oHte",
        "outputId": "3de44136-d82e-460b-927c-f30fbc331863"
      },
      "outputs": [],
      "source": [
        "plt.imshow(at.mean(axis=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zz_5dAx4t7Mo",
      "metadata": {
        "id": "zz_5dAx4t7Mo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
