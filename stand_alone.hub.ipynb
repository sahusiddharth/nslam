{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/sk-wiz/dino_deitsmall16_pretrain.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14256/895654900.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Load the state dict from the .pth file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpth_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Now you can use the state_dict as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/sk-wiz/dino_deitsmall16_pretrain.pth'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "/home/sk-wiz/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth is a zip archive (did you mean to use torch.jit.load()?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 8: 'tStorage'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2288\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2289\u001b[0;31m                 \u001b[0mtarinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2290\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[0;34m(cls, tarfile)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[0;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mchksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m148\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m156\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid header\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[1;32m   1592\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown compression type %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[0;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[1;32m   1485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2300\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2301\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2302\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mEmptyHeaderError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_16674/1509689923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m torch.hub.load_state_dict_from_url(\n\u001b[0;32m----> 3\u001b[0;31m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"https://dl.fbaipublicfiles.com/dino/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             ).keys()\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/hub.py\u001b[0m in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_legacy_zip_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_zip_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcached_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0;31m# .zip is used for torch.jit.save and will throw an un-pickling error here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} is a zip archive (did you mean to use torch.jit.load()?)\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m             \u001b[0;31m# if not a tarfile, reset file offset and proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: /home/sk-wiz/.cache/torch/hub/checkpoints/dino_deitsmall16_pretrain.pth is a zip archive (did you mean to use torch.jit.load()?)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.load_state_dict_from_url(\n",
    "                url=\"https://dl.fbaipublicfiles.com/dino/\" + \"dino_deitsmall16_pretrain/dino_deitsmall16_pretrain.pth\"\n",
    "            ).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### new file baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MAP_LOCATION' from 'torch.serialization' (/home/sk-wiz/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10778/1719478922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murlparse\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserialization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMAP_LOCATION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_Faketqdm\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# type: ignore[no-redef]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MAP_LOCATION' from 'torch.serialization' (/home/sk-wiz/miniconda3/envs/nslam/lib/python3.7/site-packages/torch/serialization.py)"
     ]
    }
   ],
   "source": [
    "# import contextlib\n",
    "# import errno\n",
    "# import hashlib\n",
    "# import json\n",
    "# import os\n",
    "# import re\n",
    "# import shutil\n",
    "# import sys\n",
    "# import tempfile\n",
    "# import torch\n",
    "# import uuid\n",
    "# import warnings\n",
    "# import zipfile\n",
    "# from pathlib import Path\n",
    "# from typing import Dict, Optional, Any\n",
    "# from urllib.error import HTTPError, URLError\n",
    "# from urllib.request import urlopen, Request\n",
    "# from urllib.parse import urlparse  # noqa: F401\n",
    "# from torch.serialization import MAP_LOCATION\n",
    "\n",
    "# class _Faketqdm:  # type: ignore[no-redef]\n",
    "\n",
    "#     def __init__(self, total=None, disable=False,\n",
    "#                  unit=None, *args, **kwargs):\n",
    "#         self.total = total\n",
    "#         self.disable = disable\n",
    "#         self.n = 0\n",
    "#         # Ignore all extra *args and **kwargs lest you want to reinvent tqdm\n",
    "\n",
    "#     def update(self, n):\n",
    "#         if self.disable:\n",
    "#             return\n",
    "\n",
    "#         self.n += n\n",
    "#         if self.total is None:\n",
    "#             sys.stderr.write(f\"\\r{self.n:.1f} bytes\")\n",
    "#         else:\n",
    "#             sys.stderr.write(f\"\\r{100 * self.n / float(self.total):.1f}%\")\n",
    "#         sys.stderr.flush()\n",
    "\n",
    "#     # Don't bother implementing; use real tqdm if you want\n",
    "#     def set_description(self, *args, **kwargs):\n",
    "#         pass\n",
    "\n",
    "#     def write(self, s):\n",
    "#         sys.stderr.write(f\"{s}\\n\")\n",
    "\n",
    "#     def close(self):\n",
    "#         self.disable = True\n",
    "\n",
    "#     def __enter__(self):\n",
    "#         return self\n",
    "\n",
    "#     def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "#         if self.disable:\n",
    "#             return\n",
    "\n",
    "#         sys.stderr.write('\\n')\n",
    "\n",
    "# try:\n",
    "#     from tqdm import tqdm  # If tqdm is installed use it, otherwise use the fake wrapper\n",
    "# except ImportError:\n",
    "#     tqdm = _Faketqdm\n",
    "\n",
    "# __all__ = [\n",
    "#     'download_url_to_file',\n",
    "#     'get_dir',\n",
    "#     'help',\n",
    "#     'list',\n",
    "#     'load',\n",
    "#     'load_state_dict_from_url',\n",
    "#     'set_dir',\n",
    "# ]\n",
    "\n",
    "# # matches bfd8deac from resnet18-bfd8deac.pth\n",
    "# HASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')\n",
    "\n",
    "# _TRUSTED_REPO_OWNERS = (\"facebookresearch\", \"facebookincubator\", \"pytorch\", \"fairinternal\")\n",
    "# ENV_GITHUB_TOKEN = 'GITHUB_TOKEN'\n",
    "# ENV_TORCH_HOME = 'TORCH_HOME'\n",
    "# ENV_XDG_CACHE_HOME = 'XDG_CACHE_HOME'\n",
    "# DEFAULT_CACHE_DIR = '~/.cache'\n",
    "# VAR_DEPENDENCY = 'dependencies'\n",
    "# MODULE_HUBCONF = 'hubconf.py'\n",
    "# READ_DATA_CHUNK = 128 * 1024\n",
    "# _hub_dir: Optional[str] = None\n",
    "\n",
    "\n",
    "# @contextlib.contextmanager\n",
    "# def _add_to_sys_path(path):\n",
    "#     sys.path.insert(0, path)\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         sys.path.remove(path)\n",
    "\n",
    "\n",
    "# # Copied from tools/shared/module_loader to be included in torch package\n",
    "# def _import_module(name, path):\n",
    "#     import importlib.util\n",
    "#     from importlib.abc import Loader\n",
    "#     spec = importlib.util.spec_from_file_location(name, path)\n",
    "#     assert spec is not None\n",
    "#     module = importlib.util.module_from_spec(spec)\n",
    "#     assert isinstance(spec.loader, Loader)\n",
    "#     spec.loader.exec_module(module)\n",
    "#     return module\n",
    "\n",
    "\n",
    "# def _remove_if_exists(path):\n",
    "#     if os.path.exists(path):\n",
    "#         if os.path.isfile(path):\n",
    "#             os.remove(path)\n",
    "#         else:\n",
    "#             shutil.rmtree(path)\n",
    "\n",
    "\n",
    "# def _git_archive_link(repo_owner, repo_name, ref):\n",
    "#     # See https://docs.github.com/en/rest/reference/repos#download-a-repository-archive-zip\n",
    "#     return f\"https://github.com/{repo_owner}/{repo_name}/zipball/{ref}\"\n",
    "\n",
    "\n",
    "# def _load_attr_from_module(module, func_name):\n",
    "#     # Check if callable is defined in the module\n",
    "#     if func_name not in dir(module):\n",
    "#         return None\n",
    "#     return getattr(module, func_name)\n",
    "\n",
    "\n",
    "# def _get_torch_home():\n",
    "#     torch_home = os.path.expanduser(\n",
    "#         os.getenv(ENV_TORCH_HOME,\n",
    "#                   os.path.join(os.getenv(ENV_XDG_CACHE_HOME,\n",
    "#                                          DEFAULT_CACHE_DIR), 'torch')))\n",
    "#     return torch_home\n",
    "\n",
    "\n",
    "# def _parse_repo_info(github):\n",
    "#     if ':' in github:\n",
    "#         repo_info, ref = github.split(':')\n",
    "#     else:\n",
    "#         repo_info, ref = github, None\n",
    "#     repo_owner, repo_name = repo_info.split('/')\n",
    "\n",
    "#     if ref is None:\n",
    "#         # The ref wasn't specified by the user, so we need to figure out the\n",
    "#         # default branch: main or master. Our assumption is that if main exists\n",
    "#         # then it's the default branch, otherwise it's master.\n",
    "#         try:\n",
    "#             with urlopen(f\"https://github.com/{repo_owner}/{repo_name}/tree/main/\"):\n",
    "#                 ref = 'main'\n",
    "#         except HTTPError as e:\n",
    "#             if e.code == 404:\n",
    "#                 ref = 'master'\n",
    "#             else:\n",
    "#                 raise\n",
    "#         except URLError as e:\n",
    "#             # No internet connection, need to check for cache as last resort\n",
    "#             for possible_ref in (\"main\", \"master\"):\n",
    "#                 if os.path.exists(f\"{get_dir()}/{repo_owner}_{repo_name}_{possible_ref}\"):\n",
    "#                     ref = possible_ref\n",
    "#                     break\n",
    "#             if ref is None:\n",
    "#                 raise RuntimeError(\n",
    "#                     \"It looks like there is no internet connection and the \"\n",
    "#                     f\"repo could not be found in the cache ({get_dir()})\"\n",
    "#                 ) from e\n",
    "#     return repo_owner, repo_name, ref\n",
    "\n",
    "\n",
    "# def _read_url(url):\n",
    "#     with urlopen(url) as r:\n",
    "#         return r.read().decode(r.headers.get_content_charset('utf-8'))\n",
    "\n",
    "\n",
    "# def _validate_not_a_forked_repo(repo_owner, repo_name, ref):\n",
    "#     # Use urlopen to avoid depending on local git.\n",
    "#     headers = {'Accept': 'application/vnd.github.v3+json'}\n",
    "#     token = os.environ.get(ENV_GITHUB_TOKEN)\n",
    "#     if token is not None:\n",
    "#         headers['Authorization'] = f'token {token}'\n",
    "#     for url_prefix in (\n",
    "#             f'https://api.github.com/repos/{repo_owner}/{repo_name}/branches',\n",
    "#             f'https://api.github.com/repos/{repo_owner}/{repo_name}/tags'):\n",
    "#         page = 0\n",
    "#         while True:\n",
    "#             page += 1\n",
    "#             url = f'{url_prefix}?per_page=100&page={page}'\n",
    "#             response = json.loads(_read_url(Request(url, headers=headers)))\n",
    "#             # Empty response means no more data to process\n",
    "#             if not response:\n",
    "#                 break\n",
    "#             for br in response:\n",
    "#                 if br['name'] == ref or br['commit']['sha'].startswith(ref):\n",
    "#                     return\n",
    "\n",
    "#     raise ValueError(f'Cannot find {ref} in https://github.com/{repo_owner}/{repo_name}. '\n",
    "#                      'If it\\'s a commit from a forked repo, please call hub.load() with forked repo directly.')\n",
    "\n",
    "\n",
    "# def _get_cache_or_reload(github, force_reload, trust_repo, calling_fn, verbose=True, skip_validation=False):\n",
    "#     # Setup hub_dir to save downloaded files\n",
    "#     hub_dir = get_dir()\n",
    "#     os.makedirs(hub_dir, exist_ok=True)\n",
    "#     # Parse github repo information\n",
    "#     repo_owner, repo_name, ref = _parse_repo_info(github)\n",
    "#     # Github allows branch name with slash '/',\n",
    "#     # this causes confusion with path on both Linux and Windows.\n",
    "#     # Backslash is not allowed in Github branch name so no need to\n",
    "#     # to worry about it.\n",
    "#     normalized_br = ref.replace('/', '_')\n",
    "#     # Github renames folder repo-v1.x.x to repo-1.x.x\n",
    "#     # We don't know the repo name before downloading the zip file\n",
    "#     # and inspect name from it.\n",
    "#     # To check if cached repo exists, we need to normalize folder names.\n",
    "#     owner_name_branch = '_'.join([repo_owner, repo_name, normalized_br])\n",
    "#     repo_dir = os.path.join(hub_dir, owner_name_branch)\n",
    "#     # Check that the repo is in the trusted list\n",
    "#     _check_repo_is_trusted(repo_owner, repo_name, owner_name_branch, trust_repo=trust_repo, calling_fn=calling_fn)\n",
    "\n",
    "#     use_cache = (not force_reload) and os.path.exists(repo_dir)\n",
    "\n",
    "#     if use_cache:\n",
    "#         if verbose:\n",
    "#             sys.stderr.write(f'Using cache found in {repo_dir}\\n')\n",
    "#     else:\n",
    "#         # Validate the tag/branch is from the original repo instead of a forked repo\n",
    "#         if not skip_validation:\n",
    "#             _validate_not_a_forked_repo(repo_owner, repo_name, ref)\n",
    "\n",
    "#         cached_file = os.path.join(hub_dir, normalized_br + '.zip')\n",
    "#         _remove_if_exists(cached_file)\n",
    "\n",
    "#         try:\n",
    "#             url = _git_archive_link(repo_owner, repo_name, ref)\n",
    "#             sys.stderr.write(f'Downloading: \\\"{url}\\\" to {cached_file}\\n')\n",
    "#             download_url_to_file(url, cached_file, progress=False)\n",
    "#         except HTTPError as err:\n",
    "#             if err.code == 300:\n",
    "#                 # Getting a 300 Multiple Choices error likely means that the ref is both a tag and a branch\n",
    "#                 # in the repo. This can be disambiguated by explicitely using refs/heads/ or refs/tags\n",
    "#                 # See https://git-scm.com/book/en/v2/Git-Internals-Git-References\n",
    "#                 # Here, we do the same as git: we throw a warning, and assume the user wanted the branch\n",
    "#                 warnings.warn(\n",
    "#                     f\"The ref {ref} is ambiguous. Perhaps it is both a tag and a branch in the repo? \"\n",
    "#                     \"Torchhub will now assume that it's a branch. \"\n",
    "#                     \"You can disambiguate tags and branches by explicitly passing refs/heads/branch_name or \"\n",
    "#                     \"refs/tags/tag_name as the ref. That might require using skip_validation=True.\"\n",
    "#                 )\n",
    "#                 disambiguated_branch_ref = f\"refs/heads/{ref}\"\n",
    "#                 url = _git_archive_link(repo_owner, repo_name, ref=disambiguated_branch_ref)\n",
    "#                 download_url_to_file(url, cached_file, progress=False)\n",
    "#             else:\n",
    "#                 raise\n",
    "\n",
    "#         with zipfile.ZipFile(cached_file) as cached_zipfile:\n",
    "#             extraced_repo_name = cached_zipfile.infolist()[0].filename\n",
    "#             extracted_repo = os.path.join(hub_dir, extraced_repo_name)\n",
    "#             _remove_if_exists(extracted_repo)\n",
    "#             # Unzip the code and rename the base folder\n",
    "#             cached_zipfile.extractall(hub_dir)\n",
    "\n",
    "#         _remove_if_exists(cached_file)\n",
    "#         _remove_if_exists(repo_dir)\n",
    "#         shutil.move(extracted_repo, repo_dir)  # rename the repo\n",
    "\n",
    "#     return repo_dir\n",
    "\n",
    "\n",
    "# def _check_repo_is_trusted(repo_owner, repo_name, owner_name_branch, trust_repo, calling_fn=\"load\"):\n",
    "#     hub_dir = get_dir()\n",
    "#     filepath = os.path.join(hub_dir, \"trusted_list\")\n",
    "\n",
    "#     if not os.path.exists(filepath):\n",
    "#         Path(filepath).touch()\n",
    "#     with open(filepath) as file:\n",
    "#         trusted_repos = tuple(line.strip() for line in file)\n",
    "\n",
    "#     # To minimize friction of introducing the new trust_repo mechanism, we consider that\n",
    "#     # if a repo was already downloaded by torchhub, then it is already trusted (even if it's not in the allowlist)\n",
    "#     trusted_repos_legacy = next(os.walk(hub_dir))[1]\n",
    "\n",
    "#     owner_name = '_'.join([repo_owner, repo_name])\n",
    "#     is_trusted = (\n",
    "#         owner_name in trusted_repos\n",
    "#         or owner_name_branch in trusted_repos_legacy\n",
    "#         or repo_owner in _TRUSTED_REPO_OWNERS\n",
    "#     )\n",
    "\n",
    "#     # TODO: Remove `None` option in 2.0 and change the default to \"check\"\n",
    "#     if trust_repo is None:\n",
    "#         if not is_trusted:\n",
    "#             warnings.warn(\n",
    "#                 \"You are about to download and run code from an untrusted repository. In a future release, this won't \"\n",
    "#                 \"be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., \"\n",
    "#                 \"trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, \"\n",
    "#                 f\"or {calling_fn}(..., trust_repo=True), which will assume that the prompt is to be answered with \"\n",
    "#                 f\"'yes'. You can also use {calling_fn}(..., trust_repo='check') which will only prompt for \"\n",
    "#                 f\"confirmation if the repo is not already trusted. This will eventually be the default behaviour\")\n",
    "#         return\n",
    "\n",
    "#     if (trust_repo is False) or (trust_repo == \"check\" and not is_trusted):\n",
    "#         response = input(\n",
    "#             f\"The repository {owner_name} does not belong to the list of trusted repositories and as such cannot be downloaded. \"\n",
    "#             \"Do you trust this repository and wish to add it to the trusted list of repositories (y/N)?\")\n",
    "#         if response.lower() in (\"y\", \"yes\"):\n",
    "#             if is_trusted:\n",
    "#                 print(\"The repository is already trusted.\")\n",
    "#         elif response.lower() in (\"n\", \"no\", \"\"):\n",
    "#             raise Exception(\"Untrusted repository.\")  # noqa: TRY002\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unrecognized response {response}.\")\n",
    "\n",
    "#     # At this point we're sure that the user trusts the repo (or wants to trust it)\n",
    "#     if not is_trusted:\n",
    "#         with open(filepath, \"a\") as file:\n",
    "#             file.write(owner_name + \"\\n\")\n",
    "\n",
    "\n",
    "# def _check_module_exists(name):\n",
    "#     import importlib.util\n",
    "#     return importlib.util.find_spec(name) is not None\n",
    "\n",
    "\n",
    "# def _check_dependencies(m):\n",
    "#     dependencies = _load_attr_from_module(m, VAR_DEPENDENCY)\n",
    "\n",
    "#     if dependencies is not None:\n",
    "#         missing_deps = [pkg for pkg in dependencies if not _check_module_exists(pkg)]\n",
    "#         if len(missing_deps):\n",
    "#             raise RuntimeError(f\"Missing dependencies: {', '.join(missing_deps)}\")\n",
    "\n",
    "\n",
    "# def _load_entry_from_hubconf(m, model):\n",
    "#     if not isinstance(model, str):\n",
    "#         raise ValueError('Invalid input: model should be a string of function name')\n",
    "\n",
    "#     # Note that if a missing dependency is imported at top level of hubconf, it will\n",
    "#     # throw before this function. It's a chicken and egg situation where we have to\n",
    "#     # load hubconf to know what're the dependencies, but to import hubconf it requires\n",
    "#     # a missing package. This is fine, Python will throw proper error message for users.\n",
    "#     _check_dependencies(m)\n",
    "\n",
    "#     func = _load_attr_from_module(m, model)\n",
    "\n",
    "#     if func is None or not callable(func):\n",
    "#         raise RuntimeError(f'Cannot find callable {model} in hubconf')\n",
    "\n",
    "#     return func\n",
    "\n",
    "\n",
    "# def get_dir():\n",
    "#     r\"\"\"\n",
    "#     Get the Torch Hub cache directory used for storing downloaded models & weights.\n",
    "\n",
    "#     If :func:`~torch.hub.set_dir` is not called, default path is ``$TORCH_HOME/hub`` where\n",
    "#     environment variable ``$TORCH_HOME`` defaults to ``$XDG_CACHE_HOME/torch``.\n",
    "#     ``$XDG_CACHE_HOME`` follows the X Design Group specification of the Linux\n",
    "#     filesystem layout, with a default value ``~/.cache`` if the environment\n",
    "#     variable is not set.\n",
    "#     \"\"\"\n",
    "#     # Issue warning to move data if old env is set\n",
    "#     if os.getenv('TORCH_HUB'):\n",
    "#         warnings.warn('TORCH_HUB is deprecated, please use env TORCH_HOME instead')\n",
    "\n",
    "#     if _hub_dir is not None:\n",
    "#         return _hub_dir\n",
    "#     return os.path.join(_get_torch_home(), 'hub')\n",
    "\n",
    "\n",
    "# def set_dir(d):\n",
    "#     r\"\"\"\n",
    "#     Optionally set the Torch Hub directory used to save downloaded models & weights.\n",
    "\n",
    "#     Args:\n",
    "#         d (str): path to a local folder to save downloaded models & weights.\n",
    "#     \"\"\"\n",
    "#     global _hub_dir\n",
    "#     _hub_dir = os.path.expanduser(d)\n",
    "\n",
    "\n",
    "# def list(github, force_reload=False, skip_validation=False, trust_repo=None, verbose=True):\n",
    "#     r\"\"\"\n",
    "#     List all callable entrypoints available in the repo specified by ``github``.\n",
    "\n",
    "#     Args:\n",
    "#         github (str): a string with format \"repo_owner/repo_name[:ref]\" with an optional\n",
    "#             ref (tag or branch). If ``ref`` is not specified, the default branch is assumed to be ``main`` if\n",
    "#             it exists, and otherwise ``master``.\n",
    "#             Example: 'pytorch/vision:0.10'\n",
    "#         force_reload (bool, optional): whether to discard the existing cache and force a fresh download.\n",
    "#             Default is ``False``.\n",
    "#         skip_validation (bool, optional): if ``False``, torchhub will check that the branch or commit\n",
    "#             specified by the ``github`` argument properly belongs to the repo owner. This will make\n",
    "#             requests to the GitHub API; you can specify a non-default GitHub token by setting the\n",
    "#             ``GITHUB_TOKEN`` environment variable. Default is ``False``.\n",
    "#         trust_repo (bool, str or None): ``\"check\"``, ``True``, ``False`` or ``None``.\n",
    "#             This parameter was introduced in v1.12 and helps ensuring that users\n",
    "#             only run code from repos that they trust.\n",
    "\n",
    "#             - If ``False``, a prompt will ask the user whether the repo should\n",
    "#               be trusted.\n",
    "#             - If ``True``, the repo will be added to the trusted list and loaded\n",
    "#               without requiring explicit confirmation.\n",
    "#             - If ``\"check\"``, the repo will be checked against the list of\n",
    "#               trusted repos in the cache. If it is not present in that list, the\n",
    "#               behaviour will fall back onto the ``trust_repo=False`` option.\n",
    "#             - If ``None``: this will raise a warning, inviting the user to set\n",
    "#               ``trust_repo`` to either ``False``, ``True`` or ``\"check\"``. This\n",
    "#               is only present for backward compatibility and will be removed in\n",
    "#               v2.0.\n",
    "\n",
    "#             Default is ``None`` and will eventually change to ``\"check\"`` in v2.0.\n",
    "#         verbose (bool, optional): If ``False``, mute messages about hitting\n",
    "#             local caches. Note that the message about first download cannot be\n",
    "#             muted. Default is ``True``.\n",
    "\n",
    "#     Returns:\n",
    "#         list: The available callables entrypoint\n",
    "\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB)\n",
    "#         >>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)\n",
    "#     \"\"\"\n",
    "#     repo_dir = _get_cache_or_reload(github, force_reload, trust_repo, \"list\", verbose=verbose,\n",
    "#                                     skip_validation=skip_validation)\n",
    "\n",
    "#     with _add_to_sys_path(repo_dir):\n",
    "#         hubconf_path = os.path.join(repo_dir, MODULE_HUBCONF)\n",
    "#         hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n",
    "\n",
    "#     # We take functions starts with '_' as internal helper functions\n",
    "#     entrypoints = [f for f in dir(hub_module) if callable(getattr(hub_module, f)) and not f.startswith('_')]\n",
    "\n",
    "#     return entrypoints\n",
    "\n",
    "\n",
    "# def help(github, model, force_reload=False, skip_validation=False, trust_repo=None):\n",
    "#     r\"\"\"\n",
    "#     Show the docstring of entrypoint ``model``.\n",
    "\n",
    "#     Args:\n",
    "#         github (str): a string with format <repo_owner/repo_name[:ref]> with an optional\n",
    "#             ref (a tag or a branch). If ``ref`` is not specified, the default branch is assumed\n",
    "#             to be ``main`` if it exists, and otherwise ``master``.\n",
    "#             Example: 'pytorch/vision:0.10'\n",
    "#         model (str): a string of entrypoint name defined in repo's ``hubconf.py``\n",
    "#         force_reload (bool, optional): whether to discard the existing cache and force a fresh download.\n",
    "#             Default is ``False``.\n",
    "#         skip_validation (bool, optional): if ``False``, torchhub will check that the ref\n",
    "#             specified by the ``github`` argument properly belongs to the repo owner. This will make\n",
    "#             requests to the GitHub API; you can specify a non-default GitHub token by setting the\n",
    "#             ``GITHUB_TOKEN`` environment variable. Default is ``False``.\n",
    "#         trust_repo (bool, str or None): ``\"check\"``, ``True``, ``False`` or ``None``.\n",
    "#             This parameter was introduced in v1.12 and helps ensuring that users\n",
    "#             only run code from repos that they trust.\n",
    "\n",
    "#             - If ``False``, a prompt will ask the user whether the repo should\n",
    "#               be trusted.\n",
    "#             - If ``True``, the repo will be added to the trusted list and loaded\n",
    "#               without requiring explicit confirmation.\n",
    "#             - If ``\"check\"``, the repo will be checked against the list of\n",
    "#               trusted repos in the cache. If it is not present in that list, the\n",
    "#               behaviour will fall back onto the ``trust_repo=False`` option.\n",
    "#             - If ``None``: this will raise a warning, inviting the user to set\n",
    "#               ``trust_repo`` to either ``False``, ``True`` or ``\"check\"``. This\n",
    "#               is only present for backward compatibility and will be removed in\n",
    "#               v2.0.\n",
    "\n",
    "#             Default is ``None`` and will eventually change to ``\"check\"`` in v2.0.\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB)\n",
    "#         >>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))\n",
    "#     \"\"\"\n",
    "#     repo_dir = _get_cache_or_reload(github, force_reload, trust_repo, \"help\", verbose=True,\n",
    "#                                     skip_validation=skip_validation)\n",
    "\n",
    "#     with _add_to_sys_path(repo_dir):\n",
    "#         hubconf_path = os.path.join(repo_dir, MODULE_HUBCONF)\n",
    "#         hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n",
    "\n",
    "#     entry = _load_entry_from_hubconf(hub_module, model)\n",
    "\n",
    "#     return entry.__doc__\n",
    "\n",
    "\n",
    "# def load(repo_or_dir, model, *args, source='github', trust_repo=None, force_reload=False, verbose=True,\n",
    "#          skip_validation=False,\n",
    "#          **kwargs):\n",
    "#     r\"\"\"\n",
    "#     Load a model from a github repo or a local directory.\n",
    "\n",
    "#     Note: Loading a model is the typical use case, but this can also be used to\n",
    "#     for loading other objects such as tokenizers, loss functions, etc.\n",
    "\n",
    "#     If ``source`` is 'github', ``repo_or_dir`` is expected to be\n",
    "#     of the form ``repo_owner/repo_name[:ref]`` with an optional\n",
    "#     ref (a tag or a branch).\n",
    "\n",
    "#     If ``source`` is 'local', ``repo_or_dir`` is expected to be a\n",
    "#     path to a local directory.\n",
    "\n",
    "#     Args:\n",
    "#         repo_or_dir (str): If ``source`` is 'github',\n",
    "#             this should correspond to a github repo with format ``repo_owner/repo_name[:ref]`` with\n",
    "#             an optional ref (tag or branch), for example 'pytorch/vision:0.10'. If ``ref`` is not specified,\n",
    "#             the default branch is assumed to be ``main`` if it exists, and otherwise ``master``.\n",
    "#             If ``source`` is 'local'  then it should be a path to a local directory.\n",
    "#         model (str): the name of a callable (entrypoint) defined in the\n",
    "#             repo/dir's ``hubconf.py``.\n",
    "#         *args (optional): the corresponding args for callable ``model``.\n",
    "#         source (str, optional): 'github' or 'local'. Specifies how\n",
    "#             ``repo_or_dir`` is to be interpreted. Default is 'github'.\n",
    "#         trust_repo (bool, str or None): ``\"check\"``, ``True``, ``False`` or ``None``.\n",
    "#             This parameter was introduced in v1.12 and helps ensuring that users\n",
    "#             only run code from repos that they trust.\n",
    "\n",
    "#             - If ``False``, a prompt will ask the user whether the repo should\n",
    "#               be trusted.\n",
    "#             - If ``True``, the repo will be added to the trusted list and loaded\n",
    "#               without requiring explicit confirmation.\n",
    "#             - If ``\"check\"``, the repo will be checked against the list of\n",
    "#               trusted repos in the cache. If it is not present in that list, the\n",
    "#               behaviour will fall back onto the ``trust_repo=False`` option.\n",
    "#             - If ``None``: this will raise a warning, inviting the user to set\n",
    "#               ``trust_repo`` to either ``False``, ``True`` or ``\"check\"``. This\n",
    "#               is only present for backward compatibility and will be removed in\n",
    "#               v2.0.\n",
    "\n",
    "#             Default is ``None`` and will eventually change to ``\"check\"`` in v2.0.\n",
    "#         force_reload (bool, optional): whether to force a fresh download of\n",
    "#             the github repo unconditionally. Does not have any effect if\n",
    "#             ``source = 'local'``. Default is ``False``.\n",
    "#         verbose (bool, optional): If ``False``, mute messages about hitting\n",
    "#             local caches. Note that the message about first download cannot be\n",
    "#             muted. Does not have any effect if ``source = 'local'``.\n",
    "#             Default is ``True``.\n",
    "#         skip_validation (bool, optional): if ``False``, torchhub will check that the branch or commit\n",
    "#             specified by the ``github`` argument properly belongs to the repo owner. This will make\n",
    "#             requests to the GitHub API; you can specify a non-default GitHub token by setting the\n",
    "#             ``GITHUB_TOKEN`` environment variable. Default is ``False``.\n",
    "#         **kwargs (optional): the corresponding kwargs for callable ``model``.\n",
    "\n",
    "#     Returns:\n",
    "#         The output of the ``model`` callable when called with the given\n",
    "#         ``*args`` and ``**kwargs``.\n",
    "\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB)\n",
    "#         >>> # from a github repo\n",
    "#         >>> repo = 'pytorch/vision'\n",
    "#         >>> model = torch.hub.load(repo, 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V1')\n",
    "#         >>> # from a local directory\n",
    "#         >>> path = '/some/local/path/pytorch/vision'\n",
    "#         >>> # xdoctest: +SKIP\n",
    "#         >>> model = torch.hub.load(path, 'resnet50', weights='ResNet50_Weights.DEFAULT')\n",
    "#     \"\"\"\n",
    "#     source = source.lower()\n",
    "\n",
    "#     if source not in ('github', 'local'):\n",
    "#         raise ValueError(\n",
    "#             f'Unknown source: \"{source}\". Allowed values: \"github\" | \"local\".')\n",
    "\n",
    "#     if source == 'github':\n",
    "#         repo_or_dir = _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \"load\",\n",
    "#                                            verbose=verbose, skip_validation=skip_validation)\n",
    "\n",
    "#     model = _load_local(repo_or_dir, model, *args, **kwargs)\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def _load_local(hubconf_dir, model, *args, **kwargs):\n",
    "#     r\"\"\"\n",
    "#     Load a model from a local directory with a ``hubconf.py``.\n",
    "\n",
    "#     Args:\n",
    "#         hubconf_dir (str): path to a local directory that contains a\n",
    "#             ``hubconf.py``.\n",
    "#         model (str): name of an entrypoint defined in the directory's\n",
    "#             ``hubconf.py``.\n",
    "#         *args (optional): the corresponding args for callable ``model``.\n",
    "#         **kwargs (optional): the corresponding kwargs for callable ``model``.\n",
    "\n",
    "#     Returns:\n",
    "#         a single model with corresponding pretrained weights.\n",
    "\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +SKIP(\"stub local path\")\n",
    "#         >>> path = '/some/local/path/pytorch/vision'\n",
    "#         >>> model = _load_local(path, 'resnet50', weights='ResNet50_Weights.IMAGENET1K_V1')\n",
    "#     \"\"\"\n",
    "#     with _add_to_sys_path(hubconf_dir):\n",
    "#         hubconf_path = os.path.join(hubconf_dir, MODULE_HUBCONF)\n",
    "#         hub_module = _import_module(MODULE_HUBCONF, hubconf_path)\n",
    "\n",
    "#         entry = _load_entry_from_hubconf(hub_module, model)\n",
    "#         model = entry(*args, **kwargs)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def download_url_to_file(url: str, dst: str, hash_prefix: Optional[str] = None,\n",
    "#                          progress: bool = True) -> None:\n",
    "#     r\"\"\"Download object at the given URL to a local path.\n",
    "\n",
    "#     Args:\n",
    "#         url (str): URL of the object to download\n",
    "#         dst (str): Full path where object will be saved, e.g. ``/tmp/temporary_file``\n",
    "#         hash_prefix (str, optional): If not None, the SHA256 downloaded file should start with ``hash_prefix``.\n",
    "#             Default: None\n",
    "#         progress (bool, optional): whether or not to display a progress bar to stderr\n",
    "#             Default: True\n",
    "\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB)\n",
    "#         >>> # xdoctest: +REQUIRES(POSIX)\n",
    "#         >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')\n",
    "\n",
    "#     \"\"\"\n",
    "#     file_size = None\n",
    "#     req = Request(url, headers={\"User-Agent\": \"torch.hub\"})\n",
    "#     u = urlopen(req)\n",
    "#     meta = u.info()\n",
    "#     if hasattr(meta, 'getheaders'):\n",
    "#         content_length = meta.getheaders(\"Content-Length\")\n",
    "#     else:\n",
    "#         content_length = meta.get_all(\"Content-Length\")\n",
    "#     if content_length is not None and len(content_length) > 0:\n",
    "#         file_size = int(content_length[0])\n",
    "\n",
    "#     # We deliberately save it in a temp file and move it after\n",
    "#     # download is complete. This prevents a local working checkpoint\n",
    "#     # being overridden by a broken download.\n",
    "#     # We deliberately do not use NamedTemporaryFile to avoid restrictive\n",
    "#     # file permissions being applied to the downloaded file.\n",
    "#     dst = os.path.expanduser(dst)\n",
    "#     for seq in range(tempfile.TMP_MAX):\n",
    "#         tmp_dst = dst + '.' + uuid.uuid4().hex + '.partial'\n",
    "#         try:\n",
    "#             f = open(tmp_dst, 'w+b')\n",
    "#         except FileExistsError:\n",
    "#             continue\n",
    "#         break\n",
    "#     else:\n",
    "#         raise FileExistsError(errno.EEXIST, 'No usable temporary file name found')\n",
    "\n",
    "#     try:\n",
    "#         if hash_prefix is not None:\n",
    "#             sha256 = hashlib.sha256()\n",
    "#         with tqdm(total=file_size, disable=not progress,\n",
    "#                   unit='B', unit_scale=True, unit_divisor=1024) as pbar:\n",
    "#             while True:\n",
    "#                 buffer = u.read(READ_DATA_CHUNK)\n",
    "#                 if len(buffer) == 0:\n",
    "#                     break\n",
    "#                 f.write(buffer)  # type: ignore[possibly-undefined]\n",
    "#                 if hash_prefix is not None:\n",
    "#                     sha256.update(buffer)  # type: ignore[possibly-undefined]\n",
    "#                 pbar.update(len(buffer))\n",
    "\n",
    "#         f.close()\n",
    "#         if hash_prefix is not None:\n",
    "#             digest = sha256.hexdigest()  # type: ignore[possibly-undefined]\n",
    "#             if digest[:len(hash_prefix)] != hash_prefix:\n",
    "#                 raise RuntimeError(f'invalid hash value (expected \"{hash_prefix}\", got \"{digest}\")')\n",
    "#         shutil.move(f.name, dst)\n",
    "#     finally:\n",
    "#         f.close()\n",
    "#         if os.path.exists(f.name):\n",
    "#             os.remove(f.name)\n",
    "\n",
    "\n",
    "# # Hub used to support automatically extracts from zipfile manually compressed by users.\n",
    "# # The legacy zip format expects only one file from torch.save() < 1.6 in the zip.\n",
    "# # We should remove this support since zipfile is now default zipfile format for torch.save().\n",
    "# def _is_legacy_zip_format(filename: str) -> bool:\n",
    "#     if zipfile.is_zipfile(filename):\n",
    "#         infolist = zipfile.ZipFile(filename).infolist()\n",
    "#         return len(infolist) == 1 and not infolist[0].is_dir()\n",
    "#     return False\n",
    "\n",
    "\n",
    "# def _legacy_zip_load(filename: str, model_dir: str, map_location: MAP_LOCATION, weights_only: bool) -> Dict[str, Any]:\n",
    "#     warnings.warn('Falling back to the old format < 1.6. This support will be '\n",
    "#                   'deprecated in favor of default zipfile format introduced in 1.6. '\n",
    "#                   'Please redo torch.save() to save it in the new zipfile format.')\n",
    "#     # Note: extractall() defaults to overwrite file if exists. No need to clean up beforehand.\n",
    "#     #       We deliberately don't handle tarfile here since our legacy serialization format was in tar.\n",
    "#     #       E.g. resnet18-5c106cde.pth which is widely used.\n",
    "#     with zipfile.ZipFile(filename) as f:\n",
    "#         members = f.infolist()\n",
    "#         if len(members) != 1:\n",
    "#             raise RuntimeError('Only one file(not dir) is allowed in the zipfile')\n",
    "#         f.extractall(model_dir)\n",
    "#         extraced_name = members[0].filename\n",
    "#         extracted_file = os.path.join(model_dir, extraced_name)\n",
    "#     return torch.load(extracted_file, map_location=map_location, weights_only=weights_only)\n",
    "\n",
    "\n",
    "# def load_state_dict_from_url(\n",
    "#     url: str,\n",
    "#     model_dir: Optional[str] = None,\n",
    "#     map_location: MAP_LOCATION = None,\n",
    "#     progress: bool = True,\n",
    "#     check_hash: bool = False,\n",
    "#     file_name: Optional[str] = None,\n",
    "#     weights_only: bool = False,\n",
    "# ) -> Dict[str, Any]:\n",
    "#     r\"\"\"Loads the Torch serialized object at the given URL.\n",
    "\n",
    "#     If downloaded file is a zip file, it will be automatically\n",
    "#     decompressed.\n",
    "\n",
    "#     If the object is already present in `model_dir`, it's deserialized and\n",
    "#     returned.\n",
    "#     The default value of ``model_dir`` is ``<hub_dir>/checkpoints`` where\n",
    "#     ``hub_dir`` is the directory returned by :func:`~torch.hub.get_dir`.\n",
    "\n",
    "#     Args:\n",
    "#         url (str): URL of the object to download\n",
    "#         model_dir (str, optional): directory in which to save the object\n",
    "#         map_location (optional): a function or a dict specifying how to remap storage locations (see torch.load)\n",
    "#         progress (bool, optional): whether or not to display a progress bar to stderr.\n",
    "#             Default: True\n",
    "#         check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention\n",
    "#             ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more\n",
    "#             digits of the SHA256 hash of the contents of the file. The hash is used to\n",
    "#             ensure unique names and to verify the contents of the file.\n",
    "#             Default: False\n",
    "#         file_name (str, optional): name for the downloaded file. Filename from ``url`` will be used if not set.\n",
    "#         weights_only(bool, optional): If True, only weights will be loaded and no complex pickled objects.\n",
    "#             Recommended for untrusted sources. See :func:`~torch.load` for more details.\n",
    "\n",
    "#     Example:\n",
    "#         >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_HUB)\n",
    "#         >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Issue warning to move data if old env is set\n",
    "#     if os.getenv('TORCH_MODEL_ZOO'):\n",
    "#         warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n",
    "\n",
    "#     if model_dir is None:\n",
    "#         hub_dir = get_dir()\n",
    "#         model_dir = os.path.join(hub_dir, 'checkpoints')\n",
    "\n",
    "#     os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#     parts = urlparse(url)\n",
    "#     filename = os.path.basename(parts.path)\n",
    "#     if file_name is not None:\n",
    "#         filename = file_name\n",
    "#     cached_file = os.path.join(model_dir, filename)\n",
    "#     if not os.path.exists(cached_file):\n",
    "#         sys.stderr.write(f'Downloading: \"{url}\" to {cached_file}\\n')\n",
    "#         hash_prefix = None\n",
    "#         if check_hash:\n",
    "#             r = HASH_REGEX.search(filename)  # r is Optional[Match[str]]\n",
    "#             hash_prefix = r.group(1) if r else None\n",
    "#         download_url_to_file(url, cached_file, hash_prefix, progress=progress)\n",
    "\n",
    "#     if _is_legacy_zip_format(cached_file):\n",
    "#         return _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
    "#     return torch.load(cached_file, map_location=map_location, weights_only=weights_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### old file baby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nslam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
